{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M-Talha-Farooqi/Machine-Learning-CourseWork/blob/main/Assignments/Assignment-3/Random_Forest_Classifier.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "576deb2c"
      },
      "source": [
        "## Random Forest Classifier Explanation\n",
        "\n",
        "This code implements a Random Forest Classifier from scratch using a custom Decision Tree Classifier.\n",
        "\n",
        "### Node Class\n",
        "\n",
        "The `Node` class represents a node in the decision tree. Each node can be either an internal node or a leaf node.\n",
        "\n",
        "- `feature_index`: The index of the feature used for splitting at this node (for internal nodes).\n",
        "- `threshold`: The threshold value for splitting at this node (for internal nodes).\n",
        "- `left`: The left child node.\n",
        "- `right`: The right child node.\n",
        "- `info_gain`: The information gain achieved by splitting at this node.\n",
        "- `value`: The predicted class value for a leaf node.\n",
        "\n",
        "### DecisionTreeClassifier Class\n",
        "\n",
        "The `DecisionTreeClassifier` class implements a single decision tree.\n",
        "\n",
        "- `__init__(self, min_samples_split=2, max_depth=2)`: Initializes the tree with minimum samples required to split a node and maximum depth of the tree.\n",
        "- `fit(self, X, Y)`: Trains the decision tree on the input data `X` and target labels `Y`. It calls the `build_tree` method to recursively build the tree.\n",
        "- `build_tree(self, dataset, curr_depth=0)`: Recursively builds the decision tree. It finds the best split at each node based on information gain and creates child nodes.\n",
        "- `calculate_leaf_value(self, Y)`: Calculates the most frequent class in the target labels `Y` for a leaf node.\n",
        "- `get_best_split(self, dataset, num_samples, num_features)`: Finds the best feature and threshold to split the data based on the maximum information gain.\n",
        "- `split(self, dataset, feature_index, threshold)`: Splits the dataset into two subsets based on the specified feature and threshold.\n",
        "- `information_gain(self, parent, l_child, r_child, mode=\"gini\")`: Calculates the information gain using either Gini impurity or entropy.\n",
        "- `entropy(self, y)`: Calculates the entropy of the target labels `y`.\n",
        "- `gini_index(self, y)`: Calculates the Gini impurity of the target labels `y`.\n",
        "- `predict(self, X)`: Predicts the class labels for the input data `X`.\n",
        "- `make_prediction(self, x, tree)`: Traverses the decision tree to predict the class label for a single data point `x`.\n",
        "\n",
        "### RandomForestClassifier Class\n",
        "\n",
        "The `RandomForestClassifier` class implements the random forest algorithm using multiple Decision Tree Classifiers.\n",
        "\n",
        "- `__init__(self, n_estimators=5, min_samples_split=2, max_depth=5)`: Initializes the random forest with the number of decision trees (`n_estimators`), minimum samples per split, and maximum depth for each tree.\n",
        "- `fit(self, X, Y)`: Trains the random forest. It creates `n_estimators` decision trees, trains each tree on a bootstrapped sample of the data, and stores them in the `trees` list.\n",
        "- `predict(self, X)`: Predicts the class labels for the input data `X`. It gets predictions from each decision tree and uses a majority vote to determine the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teOCKKb4hsS1"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A18I7AmlP8mt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01EiDg9nh1YH"
      },
      "source": [
        "# Random Forest Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uzPZ1emiHZja"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.info_gain = info_gain\n",
        "        self.value = value\n",
        "\n",
        "# ------------------------------\n",
        "# Decision Tree Classifier\n",
        "# ------------------------------\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, min_samples_split=2, max_depth=2):\n",
        "        self.root = None\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        if Y.ndim == 1:\n",
        "            Y = Y.reshape(-1, 1)\n",
        "        dataset = np.concatenate((X, Y), axis=1)\n",
        "        self.root = self.build_tree(dataset)\n",
        "\n",
        "    def build_tree(self, dataset, curr_depth=0):\n",
        "        X, Y = dataset[:, :-1], dataset[:, -1]\n",
        "        num_samples, num_features = np.shape(X)\n",
        "\n",
        "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
        "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
        "            if best_split and \"info_gain\" in best_split and best_split[\"info_gain\"] > 0:\n",
        "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1)\n",
        "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1)\n",
        "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n",
        "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
        "        leaf_value = self.calculate_leaf_value(Y)\n",
        "        return Node(value=leaf_value)\n",
        "\n",
        "\n",
        "    def calculate_leaf_value(self, Y):\n",
        "        Y = list(Y)\n",
        "        return max(Y, key=Y.count)\n",
        "\n",
        "    def get_best_split(self, dataset, num_samples, num_features):\n",
        "        best_split = {}\n",
        "        max_info_gain = -float(\"inf\")\n",
        "\n",
        "        for feature_index in range(num_features):\n",
        "            feature_values = dataset[:, feature_index]\n",
        "            possible_thresholds = np.unique(feature_values)\n",
        "\n",
        "            for threshold in possible_thresholds:\n",
        "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
        "\n",
        "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
        "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
        "                    current_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
        "\n",
        "                    if current_info_gain > max_info_gain:\n",
        "                        best_split[\"feature_index\"] = feature_index\n",
        "                        best_split[\"threshold\"] = threshold\n",
        "                        best_split[\"dataset_left\"] = dataset_left\n",
        "                        best_split[\"dataset_right\"] = dataset_right\n",
        "                        best_split[\"info_gain\"] = current_info_gain\n",
        "                        max_info_gain = current_info_gain\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def split(self, dataset, feature_index, threshold):\n",
        "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
        "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
        "        return dataset_left, dataset_right\n",
        "\n",
        "    def information_gain(self, parent, l_child, r_child, mode=\"gini\"):\n",
        "        weight_l = len(l_child) / len(parent)\n",
        "        weight_r = len(r_child) / len(parent)\n",
        "        if mode == \"gini\":\n",
        "            gain = self.gini_index(parent) - (weight_l * self.gini_index(l_child) + weight_r * self.gini_index(r_child))\n",
        "        else:\n",
        "            gain = self.entropy(parent) - (weight_l * self.entropy(l_child) + weight_r * self.entropy(r_child))\n",
        "        return gain\n",
        "\n",
        "    def entropy(self, y):\n",
        "        class_labels = np.unique(y)\n",
        "        entropy = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y)\n",
        "            entropy += -p_cls * np.log2(p_cls)\n",
        "        return entropy\n",
        "\n",
        "    def gini_index(self, y):\n",
        "        class_labels = np.unique(y)\n",
        "        gini = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y)\n",
        "            gini += p_cls ** 2\n",
        "        return 1 - gini\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self.make_prediction(x, self.root) for x in X]\n",
        "        return predictions\n",
        "\n",
        "    def make_prediction(self, x, tree):\n",
        "        if tree.value is not None:\n",
        "            return tree.value\n",
        "\n",
        "        feature_val = x[tree.feature_index]\n",
        "        if feature_val <= tree.threshold:\n",
        "            return self.make_prediction(x, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(x, tree.right)\n",
        "\n",
        "# ------------------------------\n",
        "# Random Forest Classifier\n",
        "# ------------------------------\n",
        "class RandomForestClassifier:\n",
        "    def __init__(self, n_estimators=5, min_samples_split=2, max_depth=5):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        if Y.ndim == 1:\n",
        "            Y = Y.reshape(-1, 1)\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            idxs = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, Y_sample = X[idxs], Y[idxs]\n",
        "            tree = DecisionTreeClassifier(min_samples_split=self.min_samples_split, max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, Y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "        y_pred = [Counter(row).most_common(1)[0][0] for row in tree_preds]\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5yYx9X4iJ-B"
      },
      "source": [
        "# Loading DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NCyvTYwiH6r4"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Test on Iris Dataset\n",
        "# ------------------------------\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "Y = data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PjZeJLZiEAa"
      },
      "source": [
        "# Train - Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UqYoUIlVgoAv"
      },
      "outputs": [],
      "source": [
        "# Split into train/test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5r353eciQB9"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "d9Ncz5jWgrjD"
      },
      "outputs": [],
      "source": [
        "# Train random forest\n",
        "clf = RandomForestClassifier(n_estimators=5, max_depth=5)\n",
        "clf.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlto7zFsiSnh"
      },
      "source": [
        "# Predict from Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lDs77HMKgvMb"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "preds = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZH7ewQvh_Wf"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyCo-GAogxtD",
        "outputId": "f6af8103-7790-489e-95d5-3c396f3a4897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 100.0\n"
          ]
        }
      ],
      "source": [
        "# Evaluate accuracy\n",
        "acc = accuracy_score(Y_test, preds)\n",
        "print(\"Random Forest Accuracy:\", acc*100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
